{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CKO JAR Revision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import rpy2.rinterface #ggplot tool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review TNIC-3 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hoberg and Philips TNIC3 database\n",
    "\"\"\"\n",
    "tnic = pd.read_csv('/Users/ohn0000/Project/cko/0_data/external/tnic3_data.txt', delimiter='\\t', header=0)\n",
    "tnic.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# Subset to firms with more than 20 competitors each year\n",
    "# \"\"\"\n",
    "# tnic.set_index([\"gvkey1\", \"year\", \"gvkey2\"], inplace=True, verify_integrity=True)\n",
    "# tnic_industry = tnic.groupby(level=['gvkey1', 'year']).apply(lambda x: x.nlargest(20, 'score')).reset_index(level=(0,1), drop=True)\n",
    "# tnic_industry = tnic_industry.groupby(level=['gvkey1', 'year']).filter(lambda x: x.size == 20)\n",
    "# tnic.reset_index(inplace=True)\n",
    "\n",
    "# \"\"\"\n",
    "# Save tnic_industry to a csv file\n",
    "# \"\"\"\n",
    "# tnic_industry.to_csv('/Users/ohn0000/Project/cko/2_pipeline/tnic_sub.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnic_industry = pd.read_csv('/Users/ohn0000/Project/cko/2_pipeline/tnic_sub.csv', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tnic_industry['gvkey1'] = tnic_industry['gvkey1'].apply(lambda x: str(x).zfill(6))\n",
    "# tnic_industry['gvkey2'] = tnic_industry['gvkey2'].apply(lambda x: str(x).zfill(6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remeber that _year_ in __tnic_industry__ is the base year for identifying close competitors. Accordingly, _lead1_ is the M&A year and _lead2_ is the year following M&A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Readme_tnic3.txt explains that _year_ equals the first four digits of the __compustat__ _datadate_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shift years in __tnic__ to get _lead1_ similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnic['year'] = tnic['year'] - 1\n",
    "tnic.rename(columns={'score':'score_lead1'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnic_industry = pd.merge(tnic_industry, tnic, how='left', left_on=['gvkey1', 'year', 'gvkey2'], right_on=['gvkey1', 'year', 'gvkey2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shift years one more time to get _lead2_ similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnic['year'] = tnic['year'] - 1\n",
    "tnic.rename(columns={'score_lead1':'score_lead2'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnic_industry = pd.merge(tnic_industry, tnic, how='left', left_on=['gvkey1', 'year', 'gvkey2'], right_on=['gvkey1', 'year', 'gvkey2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnic_industry.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnic_industry.set_index(['gvkey1', 'year', 'gvkey2'], inplace=True, verify_integrity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reset __tnic__ years and column name back to original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tnic['year'] = tnic['year'] + 2\n",
    "tnic.rename(columns={'score_lead2':'score'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average TNIC similarity score across 20-closest competitors.  \n",
    "Remeber that in __TNIC_ALL__ most of the scores equals to zero. The _z\\__ might be the more suitable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sim = tnic_industry.groupby(level=['gvkey1','year']).mean()\n",
    "avg_sim = avg_sim.join(tnic_industry.groupby(level=['gvkey1','year']).count().add_prefix(\"n_\"))\n",
    "avg_sim = avg_sim.join(tnic_industry.fillna(0).groupby(level=['gvkey1','year']).mean().add_prefix(\"z_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avg_sim.dropna() \n",
    "# # 54963 observations with non-missing scores\n",
    "\n",
    "# avg_sim[(avg_sim['n_score'] == 20) & (avg_sim['n_score_lead1'] == 20) & (avg_sim['n_score_lead2'] == 20)]\n",
    "# # 991 observations with all 20 competitors present in TNIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The materiality measure based on deal value will be the last resort for the IV.   \n",
    "Alternatively, 2SLS using multiple IVs is feasible.\n",
    "\n",
    "Candidates\n",
    "* Max deal value\n",
    "* Sum deal value\n",
    "* Datedif between _dateeff_ and _datadate_\n",
    "    * _dateeff_ of the first M&A\n",
    "    * _dateeff_ of the largest M&A\n",
    "    * weighted average of _dateeff_ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import previously constructed datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Materiality of M&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "material = pd.read_csv('/Users/ohn0000/Project/cko/0_data/external/materiality.csv')\n",
    "material.set_index([\"year\", \"gvkey1\"], inplace=True, verify_integrity=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M&A Disclosure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disclosure also might need additonal data collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = pd.read_csv('/Users/ohn0000/Project/cko/0_data/manual/disc.csv', parse_dates=['DATADATE'])\n",
    "disc['CIK'] = disc['CIK'].apply(lambda x: str(int(x)).zfill(10) if pd.notnull(x) else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc.rename(columns={\"GVKEY\":\"gvkey1\", \"FYEAR\":\"year\"}, inplace=True)\n",
    "disc.set_index([\"year\", \"gvkey1\"], inplace=True, verify_integrity=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manual = disc.join(material)[['DATADATE', 'CIK', 'TGTAT_ACQAT', 'TGTDVAL_ACQAT', 'MD_A', 'PROFORMA']].sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SDC and Compustat Link File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The link file is from [Michael Ewens](https://github.com/michaelewens/SDC-to-Compustat-Mapping.git). Cite papers below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "@article{phillips2013r,\n",
    "  title={R\\&D and the Incentives from Merger and Acquisition Activity},\n",
    "  author={Phillips, Gordon M and Zhdanov, Alexei},\n",
    "  journal={The Review of Financial Studies},\n",
    "  volume={26},\n",
    "  number={1},\n",
    "  pages={34--78},\n",
    "  year={2013},\n",
    "  publisher={Society for Financial Studies}\n",
    "  }\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "@article{ewensPetersWang2018,\n",
    " title={Acquisition prices and the measurement of intangible capital},\n",
    " author={Ewens, Michael and Peters, Ryan and Wang, Sean},\n",
    " journal={Working Paper}\n",
    " year={2018}\n",
    " }\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdc_link = pd.read_csv('/Users/ohn0000/Project/cko/0_data/external/dealnum_to_gvkey.csv', \n",
    "                       dtype={'DealNumber':'Int64', 'agvkey':'Int64', 'tgvkey':'Int64'}, index_col='DealNumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wrds\n",
    "# db = wrds.Connection(wrds_username = \"yaera\")\n",
    "# ma_details_desc = db.describe_table('sdc', 'ma_details').sort_values('name')\n",
    "# with pd.option_context('display.max_rows', None):\n",
    "#     print(ma_details_desc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|     Variable | Description                    |\n",
    "|:------------:|:-------------------------------|\n",
    "|bookvalue     |Target Book Value (\\$mil)       |\n",
    "|compete       |Competing Bidder (Y/N)          |\n",
    "|competecode   |Competing Bid Deal Code         |  \n",
    "|dateann       |Date Announced                  |\n",
    "|dateannest    |_dateann_ is estimated (Y/N)    | \n",
    "|dateeff       |Date Effective                  | \n",
    "|ebitltm       |Target EBIT LTM (\\$mil)         |\n",
    "|pct_cash      |Percentage of consideration paid in cash|\n",
    "|pct_other|Percentage of consideration paid in other then cash or stock|\n",
    "|pct_stk|Percentage of consideration paid in stock|\n",
    "|pct_unknown|Percentage of consideration which is unknown|\n",
    "|ptincltm|Target Pre-Tax Income LTM (\\$mil)|\n",
    "|salesltm|Target Sales LTM (\\$mil)|\n",
    "|rankval|Ranking Value incl Net Debt of Target (\\$mil)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run sql query below on _WRDS_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wrds\n",
    "# sdc_query = \"\"\"\n",
    "# select master_deal_no as DealNumber, \n",
    "#         bookvalue, \n",
    "#         compete, \n",
    "#         competecode, \n",
    "#         dateann, \n",
    "#         dateannest, \n",
    "#         dateeff, \n",
    "#         ebitltm, \n",
    "#         pct_cash,\n",
    "#         pct_other,\n",
    "#         pct_stk,\n",
    "#         pct_unknown,\n",
    "#         ptincltm,\n",
    "#         salesltm,\n",
    "#         rankval\n",
    "# from sdc.ma_details\n",
    "# where dateeff is not null \n",
    "# \"\"\"\n",
    "# # and master_deal_no in %(deal_no)s\n",
    "# sdc = db.raw_sql(sdc_query, date_cols=['dateann', 'dateeff'])\n",
    "# sdc.to_pickle('/home/upenn/yaera/sdc.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdc = pd.read_pickle('/Users/ohn0000/Project/cko/0_data/external/sdc.pkl')\n",
    "sdc.drop_duplicates('dealnumber', inplace = True)\n",
    "sdc['dealnumber'] = sdc['dealnumber'].apply(int)\n",
    "\n",
    "# clear up values and change dtype to 'float'\n",
    "for column in ['bookvalue', 'ebitltm', 'pct_cash', 'pct_other', 'pct_stk', 'pct_unknown', 'ptincltm', 'salesltm', 'rankval']:\n",
    "    sdc[column] = sdc[column].apply(lambda x: np.NaN if x == '*********' else (np.NaN if pd.isna(x) else (float(x.replace(',', '')) if isinstance(x, str) else float(x))))\n",
    "    sdc[column].astype('float16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdc_sub = pd.merge(sdc_link, sdc, left_index=True, right_on='dealnumber').drop('dealnumber', axis='columns')\n",
    "sdc_sub.index.name = 'dealnumber'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdc_sub.sort_values(['agvkey', 'dateeff'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use __compustat__ _datadate_ and gvkey to link the sdc data to the similarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wrds\n",
    "db = wrds.Connection(wrds_username = 'hohn')\n",
    "\n",
    "sdc_quary = \"\"\"\n",
    "select gvkey, datadate, fyear, cusip,  cik\n",
    "from comp.funda\n",
    "where consol = %(consol)s and indfmt in %(indfmt)s and datafmt = %(datafmt)s and popsrc = %(popsrc)s and curcd in %(curcd)s\n",
    "\"\"\"\n",
    "\n",
    "parm = {'consol':('C'), 'indfmt' : ('INDL', 'FS'), 'datafmt': ('STD'), 'popsrc' : ('D'), 'curcd' : ('USD', 'CAD')}\n",
    "\n",
    "funda = db.raw_sql(sdc_quary, params = parm, date_cols = ['datadate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funda['start'] = funda['datadate'] - pd.DateOffset(months = 12) + pd.DateOffset(days = 1)\n",
    "funda['gvkey'] = funda['gvkey'].astype('int64')\n",
    "funda.set_index('gvkey', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "funda.fyear = funda.fyear.astype('Int16')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandasql as ps\n",
    "\n",
    "sql_query = '''\n",
    "select a.*, b.datadate, b.fyear, b.cusip, b.cik\n",
    "from sdc_sub a left join funda b\n",
    "on a.agvkey = b.gvkey and a.dateeff between b.start and b.datadate\n",
    "'''\n",
    "\n",
    "newdf = ps.sqldf(sql_query, locals())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = list(newdf)\n",
    "for i in range(2, 6):\n",
    "    col.insert(i, col.pop(-1))\n",
    "newdf = newdf.loc[:,col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ['datadate', 'dateann', 'dateeff']:\n",
    "    newdf[i] = newdf[i].astype('datetime64[ns]')\n",
    "    \n",
    "newdf['year'] = newdf['datadate'].dt.year.astype('Int16')\n",
    "for i in ['fyear', 'agvkey', 'tgvkey']:\n",
    "    newdf[i] = newdf[i].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = list(newdf)\n",
    "col.insert(col.index('datadate'), col.pop(col.index('year')))\n",
    "newdf = newdf.loc[:,col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = newdf.drop_duplicates(subset='dealnumber')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf[newdf['agvkey'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf['rankval'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18994 observations with non-missing _rankval_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf['salesltm'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8055 observations with non-missing _salesltm_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(newdf['rankval'].notnull() & newdf['salesltm'].notnull())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6445 observations with both _rankval_ and _salesltm_ available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Append similarity score between acquirer and target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload.csv                                    100%  115KB 299.6KB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "upload = newdf[newdf['agvkey'].notnull() & newdf['tgvkey'].notnull()][['agvkey', 'tgvkey', 'year']].rename(columns={'agvkey':'gvkey1', 'tgvkey':'gvkey2'})\n",
    "upload.to_csv('/Users/ohn0000/Project/cko/2_pipeline/upload.csv', index=False)\n",
    "!scp /Users/ohn0000/Project/cko/2_pipeline/upload.csv $WRDS:/scratch/ou/hohn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this on wrds server. The __TNIC_All__ files should be uploaded in scratch beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The server killed the previous code that joins after combines all files. The current code instead loop over the files.\n",
    "\"\"\"\n",
    "# !cd /scratch/ou/hohn/TNIC_AllPairsDistrib\n",
    "# !cat tnicall1996.txt > tnicall_combined.txt\n",
    "# !for file in tnicall{1997..2017}.txt; do sed '1d' $file >> tnicall_combined.txt; done\n",
    "# !cd ~\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "atsim.py\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "atsim.py                                      100%  545    12.5KB/s   00:00    \n"
     ]
    }
   ],
   "source": [
    "!scp atsim.py $WRDS:~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scp $WRDS:/scratch/ou/hohn/atsim.csv /Users/ohn0000/Project/cko/2_pipeline/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = list(newdf)\n",
    "col.insert(col.index('bookvalue'), col.pop(col.index('atsim')))\n",
    "newdf = newdf.loc[:,col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-sections\n",
    "* Similarity between acquirer and target \n",
    "    - Relation stronger in diversifying\n",
    "    - Could be more of a U-shaped relation, i.e., competitors don't follow when you move far enough\n",
    "* Average value of pre-similarities between acquirer and close competitors \n",
    "    - Prediction not clear\n",
    "* M&A performance during the completed firm-year\n",
    "    - Relation stronger when M&A was more successful <-> how do we define success of an M&A?\n",
    "* Number of close competitors of the target\n",
    "    - Potential targets are candidates of future mergers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (cko)",
   "language": "python",
   "name": "cko"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
